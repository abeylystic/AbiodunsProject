{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77c696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting abiFunctions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile abiFunctions.py\n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ArrowStyle\n",
    "import copy\n",
    "from pgmpy.estimators import PC\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "from linearmodels.panel import PanelOLS\n",
    "from itertools import combinations\n",
    "import pingouin as pg\n",
    "import statsmodels.api as sm\n",
    "import geopandas\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from linearmodels.panel import PooledOLS\n",
    "\n",
    "\n",
    "# Function to calculate AIC\n",
    "def calculate_aic(n, rss, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "# Function to calculate BIC\n",
    "def calculate_bic(n, rss, k):\n",
    "    return n * np.log(rss / n) + k * np.log(n)\n",
    "\n",
    "# Function to calculate HQIC\n",
    "def calculate_hqic(n, rss, k):\n",
    "    return n * np.log(rss / n) + 2 * k * np.log(np.log(n))\n",
    "\n",
    "# Function for forward stepwise selection\n",
    "def forward_stepwise_selection(df, dependent_var, fixed_predictors, potential_predictors):\n",
    "    initial_formula = f'{dependent_var} ~ ' + ' + '.join(fixed_predictors)\n",
    "    best_model = initial_formula\n",
    "    best_aic = float('inf')\n",
    "\n",
    "    # DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['Formula', 'AIC'])\n",
    "\n",
    "    # Fit the initial model\n",
    "    model = PanelOLS.from_formula(initial_formula, df, drop_absorbed=True, check_rank=False)\n",
    "    results = model.fit()\n",
    "    n = results.nobs\n",
    "    rss = np.sum(results.resids ** 2)\n",
    "    num_params = results.params.shape[0]\n",
    "    best_aic = calculate_aic(n, rss, num_params)\n",
    "    results_df = results_df.append({'Formula': initial_formula, 'AIC': best_aic}, ignore_index=True)\n",
    "\n",
    "    # Forward stepwise selection\n",
    "    for k in range(1, len(potential_predictors) + 1):\n",
    "        for subset in combinations(potential_predictors, k):\n",
    "            formula = f'{dependent_var} ~ ' + ' + '.join(fixed_predictors + list(subset))\n",
    "            model = PanelOLS.from_formula(formula, df, drop_absorbed=True, check_rank=False)\n",
    "            results = model.fit()\n",
    "            n = results.nobs\n",
    "            rss = np.sum(results.resids ** 2)\n",
    "            num_params = results.params.shape[0]\n",
    "            aic = calculate_aic(n, rss, num_params)\n",
    "            results_df = results_df.append({'Formula': formula, 'AIC': aic}, ignore_index=True)\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_model = formula\n",
    "\n",
    "    results_df = results_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    return best_model, best_aic, results_df\n",
    "\n",
    "\n",
    "# Function to perform backward stepwise selection\n",
    "def backward_stepwise_selection(df, dependent_var, fixed_predictors, potential_predictors):\n",
    "    initial_predictors = fixed_predictors + potential_predictors\n",
    "    initial_formula = f'{dependent_var} ~ ' + ' + '.join(initial_predictors)\n",
    "    best_model = initial_formula\n",
    "    best_aic = float('inf')\n",
    "\n",
    "    # DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['Formula', 'AIC'])\n",
    "\n",
    "    # Fit the initial model\n",
    "    model = PanelOLS.from_formula(initial_formula, df, drop_absorbed=True, check_rank=False)\n",
    "    results = model.fit()\n",
    "    n = results.nobs\n",
    "    rss = np.sum(results.resids ** 2)\n",
    "    num_params = results.params.shape[0]\n",
    "    best_aic = calculate_aic(n, rss, num_params)\n",
    "    results_df = results_df.append({'Formula': initial_formula, 'AIC': best_aic}, ignore_index=True)\n",
    "\n",
    "    current_predictors = initial_predictors\n",
    "\n",
    "    # Backward stepwise selection\n",
    "    improved = True\n",
    "    while improved and len(current_predictors) > len(fixed_predictors):\n",
    "        improved = False\n",
    "        for predictor in current_predictors:\n",
    "            if predictor not in fixed_predictors:\n",
    "                candidate_predictors = [p for p in current_predictors if p != predictor]\n",
    "                candidate_formula = f'{dependent_var} ~ ' + ' + '.join(candidate_predictors)\n",
    "                model = PanelOLS.from_formula(candidate_formula, df, drop_absorbed=True, check_rank=False)\n",
    "                results = model.fit()\n",
    "                n = results.nobs\n",
    "                rss = np.sum(results.resids ** 2)\n",
    "                num_params = results.params.shape[0]\n",
    "                aic = calculate_aic(n, rss, num_params)\n",
    "                results_df = results_df.append({'Formula': candidate_formula, 'AIC': aic}, ignore_index=True)\n",
    "                if aic < best_aic:\n",
    "                    best_aic = aic\n",
    "                    best_model = candidate_formula\n",
    "                    current_predictors = candidate_predictors\n",
    "                    improved = True\n",
    "\n",
    "    results_df = results_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    return best_model, best_aic, results_df\n",
    "\n",
    "\n",
    "# def get_model_summary(formula, df):\n",
    "#     model = PanelOLS.from_formula(formula, df, drop_absorbed=True, check_rank=False)\n",
    "#     results = model.fit()\n",
    "#     return results\n",
    "\n",
    "\n",
    "# Function to plot residuals vs predicted values\n",
    "def plot_residuals_vs_predicted(results, df):\n",
    "    predicted_values = results.fitted_values\n",
    "    residuals = results.resids\n",
    "    \n",
    "    # Plot the histogram of the residuals\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    residuals.plot.hist(bins=100, ax=ax)\n",
    "    plt.title(\"Residuals Histogram\")\n",
    "    plt.show()\n",
    "#     plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(predicted_values, residuals)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Predicted Values')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Function to plot residuals vs independent variables\n",
    "def plot_residuals_vs_independent_vars(results, df, fixed_predictors, potential_predictors):\n",
    "    residuals = results.resids\n",
    "    residuals = residuals[~residuals.index.duplicated(keep='first')]  # Remove duplicates from the index\n",
    "    df['Residuals'] = residuals\n",
    "\n",
    "    for predictor in fixed_predictors + potential_predictors:\n",
    "        if predictor in df.columns:\n",
    "            aligned_residuals = residuals.reindex(df.index)\n",
    "            corr = df.corr().round(3)[predictor][\"Residuals\"]\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.scatter(df[predictor], aligned_residuals)\n",
    "            plt.xlabel(predictor)\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.title(f'Residuals vs {predictor}, Corr: {corr}')\n",
    "            plt.axhline(y=0, color='r', linestyle='--')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Function to plot residuals vs dependent variable\n",
    "def plot_residuals_vs_dependent_var(results, df, dependent_var='unem', title=\"Residuals vs Dependent Variable\"):\n",
    "    residuals = results.resids\n",
    "    residuals = residuals[~residuals.index.duplicated(keep='first')]  # Remove duplicates from the index\n",
    "    df['Residuals'] = residuals\n",
    "\n",
    "    aligned_residuals = residuals.reindex(df.index)\n",
    "    corr = df.corr().round(3)[dependent_var][\"Residuals\"]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(df[dependent_var], aligned_residuals)\n",
    "    plt.xlabel(f'Dependent Variable ({dependent_var})')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'{title}({dependent_var}), Corr: {corr}')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Function to get model summary using PanelOLS\n",
    "def get_model_summary(formula, df, model_type):\n",
    "    if model_type == \"PanelOLS\":\n",
    "        model = PanelOLS.from_formula(formula, df, drop_absorbed=True, check_rank=False)\n",
    "    elif model_type == \"PooledOLS\":\n",
    "        model = PooledOLS.from_formula(formula, df, check_rank=False)\n",
    "    results = model.fit()\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to calculate average squared correlations\n",
    "def calculate_avg_squared_correlations(results, df, independent_vars):\n",
    "    residuals = results.resids\n",
    "    squared_correlations = []\n",
    "    for var in independent_vars:\n",
    "        correlation = np.corrcoef(residuals, df[var])[0, 1]\n",
    "        squared_correlation = correlation ** 2\n",
    "        squared_correlations.append(squared_correlation)\n",
    "    avg_squared_correlation = np.mean(squared_correlations)\n",
    "    return avg_squared_correlation, squared_correlations\n",
    "\n",
    "\n",
    "# Function to build skeleton for undirected DAG\n",
    "def build_skeleton(df, undirected_graph, p_val = 0.005):    \n",
    "    def check_remaining_controls(control_vars, undirected_graph, x, y, controls_used):\n",
    "        for c_var in control_vars:\n",
    "            c_used = copy.copy(controls_used)\n",
    "            if y in undirected_graph[x]:\n",
    "                c_used.append(c_var)\n",
    "                test = pg.partial_corr(data=df, x=x, y=y, covar=c_used, method=\"pearson\")\n",
    "                if test[\"p-val\"].values[0] > p_val:\n",
    "                    undirected_graph[x].remove(y)\n",
    "                    break\n",
    "                else:\n",
    "                    remaining_controls = copy.copy(control_vars)\n",
    "                    remaining_controls.remove(c_var)\n",
    "                    check_remaining_controls(remaining_controls, undirected_graph, x, y, c_used)\n",
    "\n",
    "    for x in df.columns:\n",
    "        ys = undirected_graph[x]\n",
    "        for y in ys[:]:  # Use a slice copy to avoid modifying the list during iteration\n",
    "            if x != y:\n",
    "                test = pg.partial_corr(data=df, x=x, y=y, covar=None, method=\"pearson\")\n",
    "                if test[\"p-val\"].values[0] > p_val:\n",
    "                    undirected_graph[x].remove(y)\n",
    "                else:\n",
    "                    control_vars = [z for z in df.columns if z != y and z != x]\n",
    "                    check_remaining_controls(control_vars, undirected_graph, x, y, [])\n",
    "    return undirected_graph\n",
    "\n",
    "# Function to plot undirected DAG\n",
    "def graph_undirected_DAG(undirected_graph, df, title=\"DAG Structure\"):\n",
    "    graph = nx.DiGraph()\n",
    "    edges = []\n",
    "    edge_labels = {}\n",
    "    \n",
    "    for key in undirected_graph:\n",
    "        for key2 in undirected_graph[key]:\n",
    "            if (key2, key) not in edges:\n",
    "                edge = (key.replace(\" \", \"\\n\"), key2.replace(\" \", \"\\n\"))\n",
    "                edges.append(edge)\n",
    "\n",
    "    graph.add_edges_from(edges)\n",
    "    color_map = [\"C0\" for _ in graph]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    plt.tight_layout()\n",
    "    pos = graphviz_layout(graph)\n",
    "\n",
    "    plt.title(title, fontsize=30)\n",
    "    nx.draw_networkx(graph, pos, node_color=color_map, node_size=1500,\n",
    "                     with_labels=True, arrows=False, font_size=20,\n",
    "                     alpha=1, font_color=\"white\", ax=ax)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"g1.png\", format=\"PNG\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Function to get residuals \n",
    "residuals = {}\n",
    "def get_residuals(df, weights):\n",
    "    for y_var in df.columns:\n",
    "        X_vars = list(df.columns)\n",
    "        X_vars.remove(y_var)\n",
    "        X = df[X_vars].copy()\n",
    "        # Initial estimate should include constant\n",
    "        X[\"Constant\"] = 0\n",
    "        y = df[[y_var]]\n",
    "        model = sm.WLS(y, X, weights=weights)\n",
    "        results = model.fit()\n",
    "        residuals[\"$\\\\epsilon_{\" + y_var + \"}$\"] = results.resid\n",
    "    return pd.DataFrame(residuals)\n",
    "\n",
    "\n",
    "# Function to import geographical data\n",
    "def import_geo_data(filename, index_col = \"Date\", FIPS_name = \"FIPS\"):\n",
    "    # import county level shapefile\n",
    "    map_data = geopandas.read_file(filename = filename,                                   \n",
    "                                   index_col = index_col)\n",
    "    # rename fips code to match variable name in COVID-19 data\n",
    "    map_data.rename(columns={\"State\":\"state\"},\n",
    "                    inplace = True)\n",
    "    # Combine statefips and county fips to create a single fips value\n",
    "    # that identifies each particular county without referencing the \n",
    "    # state separately\n",
    "    map_data[FIPS_name] = map_data[\"STATEFP\"].astype(str) + \\\n",
    "        map_data[\"COUNTYFP\"].astype(str)\n",
    "    map_data[FIPS_name] = map_data[FIPS_name].astype(np.int64)\n",
    "    # set FIPS as index\n",
    "    map_data.set_index(FIPS_name, inplace=True)\n",
    "    \n",
    "    return map_data\n",
    "\n",
    "\n",
    "# Function to Analyse and compare wls and pooled regressions from a dictionary of dataframes\n",
    "def analyze_wls_pooled_models(data_cluster_dict, dependent_var, k=5, shuffle=True, random_state=None, check_rank=False, num_iterations=10):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    mse_results = []\n",
    "    model_attributes = []\n",
    "\n",
    "    for key, df in data_cluster_dict.items():\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        county_unem = df.groupby('FIPS')[dependent_var].var()\n",
    "        df['weight'] = df['FIPS'].map(lambda x: 1 / county_unem.get(x, np.nan))\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['weight'])\n",
    "\n",
    "        y = df[dependent_var]\n",
    "        X = df.drop(columns=[dependent_var, 'FIPS', 'weight', 'TimePeriod'])\n",
    "        weights = df['weight']\n",
    "\n",
    "        for use_clusters in [True, False]:\n",
    "            X_filtered = X.drop(columns=[col for col in X.columns if 'cluster' in col]) if not use_clusters else X\n",
    "\n",
    "            if 'Nominal rates' in key:\n",
    "                X_filtered = sm.add_constant(X_filtered)\n",
    "                \n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"feature\"] = X_filtered.columns\n",
    "            vif_data[\"VIF\"] = [variance_inflation_factor(X_filtered.dropna().values, i) for i in range(len(X_filtered.columns))]\n",
    "\n",
    "            avg_beta_coefficients = []\n",
    "\n",
    "            for _ in range(num_iterations):\n",
    "                kf = KFold(n_splits=k, shuffle=shuffle, random_state=random_state)\n",
    "                mse_folds = []\n",
    "\n",
    "                for train_index, test_index in kf.split(df):\n",
    "                    X_train, X_test = X_filtered.iloc[train_index], X_filtered.iloc[test_index]\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                    weights_train, weights_test = weights.iloc[train_index], weights.iloc[test_index]\n",
    "\n",
    "                    model = sm.WLS(y_train, X_train, weights=weights_train).fit()\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    mse_folds.append(mean_squared_error(y_test, y_pred))\n",
    "                    avg_beta_coefficients.append(model.params.values)\n",
    "\n",
    "                avg_mse = np.mean(mse_folds)\n",
    "                mse_results.append({'Dataset': key, 'Clusters': use_clusters, 'Model': 'WLS', 'Avg MSE': avg_mse, 'MSE Folds': mse_folds, 'R^2': model.rsquared})\n",
    "\n",
    "            avg_beta_coefficients = np.mean(avg_beta_coefficients, axis=0)\n",
    "            model_attributes.append({'Dataset': key, 'Clusters': use_clusters, 'Model': 'WLS', 'Beta Estimates': pd.Series(avg_beta_coefficients, index=model.params.index), 'R^2': model.rsquared})\n",
    "\n",
    "            y_train_pooled = y_train.reset_index()\n",
    "            y_train_pooled['TimePeriod'] = df.iloc[train_index]['TimePeriod'].values\n",
    "            y_train_pooled.set_index(['index', 'TimePeriod'], inplace=True)\n",
    "            y_train_pooled = y_train_pooled[dependent_var]\n",
    "\n",
    "            y_test_pooled = y_test.reset_index()\n",
    "            y_test_pooled['TimePeriod'] = df.iloc[test_index]['TimePeriod'].values\n",
    "            y_test_pooled.set_index(['index', 'TimePeriod'], inplace=True)\n",
    "            y_test_pooled = y_test_pooled[dependent_var]\n",
    "\n",
    "            X_train_pooled = X_train.reset_index()\n",
    "            X_train_pooled['TimePeriod'] = df.iloc[train_index]['TimePeriod'].values\n",
    "            X_train_pooled.set_index(['index', 'TimePeriod'], inplace=True)\n",
    "\n",
    "            X_test_pooled = X_test.reset_index()\n",
    "            X_test_pooled['TimePeriod'] = df.iloc[test_index]['TimePeriod'].values\n",
    "            X_test_pooled.set_index(['index', 'TimePeriod'], inplace=True)\n",
    "\n",
    "            avg_pooled_beta_coefficients = []\n",
    "\n",
    "            for _ in range(num_iterations):\n",
    "                pooled_model = PooledOLS(y_train_pooled, X_train_pooled, check_rank=check_rank).fit()\n",
    "                y_pred_pooled = pooled_model.predict(X_test_pooled)\n",
    "                mse_pooled = mean_squared_error(y_test_pooled, y_pred_pooled)\n",
    "                mse_results.append({'Dataset': key, 'Clusters': use_clusters, 'Model': 'PooledOLS', 'Avg MSE': mse_pooled, 'MSE Folds': mse_folds, 'R^2': pooled_model.rsquared})\n",
    "                avg_pooled_beta_coefficients.append(pooled_model.params.values)\n",
    "\n",
    "            avg_pooled_beta_coefficients = np.mean(avg_pooled_beta_coefficients, axis=0)\n",
    "            model_attributes.append({'Dataset': key, 'Clusters': use_clusters, 'Model': 'PooledOLS', 'Beta Estimates': pd.Series(avg_pooled_beta_coefficients, index=pooled_model.params.index), 'R^2': pooled_model.rsquared})\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    for attributes in model_attributes:\n",
    "        dataset_name = attributes['Dataset']\n",
    "        clusters = attributes['Clusters']\n",
    "        model_name = attributes['Model']\n",
    "        beta_estimates = attributes['Beta Estimates']\n",
    "        avg_mse = next(result['Avg MSE'] for result in mse_results if result['Dataset'] == dataset_name and result['Clusters'] == clusters and result['Model'] == model_name)\n",
    "        mse_folds = next(result['MSE Folds'] for result in mse_results if result['Dataset'] == dataset_name and result['Clusters'] == clusters and result['Model'] == model_name)\n",
    "        r_squared = attributes['R^2']\n",
    "        mse_series = pd.Series([r_squared, avg_mse] + mse_folds, index=[\"$R^2$\", \"Avg MSE\"] + [f\"MSE Fold {i+1}\" for i in range(len(mse_folds))])\n",
    "        combined_series = pd.concat([beta_estimates, mse_series], axis=0)\n",
    "        result_df = pd.concat([result_df, combined_series], axis=1)\n",
    "        result_df.rename(columns={result_df.columns[-1]: f\"{dataset_name} - {clusters} - {model_name}\"}, inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Function to run wls and pooledOLS without k-fold\n",
    "def wls_pooled_model_analysis(data_cluster_dict, dependent_var, random_state=None, check_rank=False):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    mse_results = []\n",
    "    model_attributes = []\n",
    "\n",
    "    for key, df in data_cluster_dict.items():\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        county_unem = df.groupby('FIPS')[dependent_var].var()\n",
    "        df['weight'] = df['FIPS'].map(lambda x: 1 / county_unem.get(x, np.nan))\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['weight'])\n",
    "\n",
    "        y = df[dependent_var]\n",
    "        X = df.drop(columns=[dependent_var, 'FIPS', 'weight', 'TimePeriod'])\n",
    "        weights = df['weight']\n",
    "\n",
    "        for use_clusters in [True, False]:\n",
    "            X_filtered = X.drop(columns=[col for col in X.columns if 'cluster' in col]) if not use_clusters else X\n",
    "\n",
    "            if 'Nominal rates' in key:\n",
    "                X_filtered = sm.add_constant(X_filtered)\n",
    "                \n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"feature\"] = X_filtered.columns\n",
    "            vif_data[\"VIF\"] = [variance_inflation_factor(X_filtered.dropna().values, i) for i in range(len(X_filtered.columns))]\n",
    "\n",
    "            model = sm.WLS(y, X_filtered, weights=weights).fit()\n",
    "            model_attributes.append({'Dataset': key, 'Clusters': use_clusters, 'Model': 'WLS', 'Beta Estimates': model.params, 'R^2': model.rsquared})\n",
    "\n",
    "            y_pooled = y.reset_index()\n",
    "            y_pooled['TimePeriod'] = df['TimePeriod'].values\n",
    "            y_pooled.set_index(['index', 'TimePeriod'], inplace=True)\n",
    "            y_pooled = y_pooled[dependent_var]\n",
    "\n",
    "            X_pooled = X_filtered.reset_index()\n",
    "            X_pooled['TimePeriod'] = df['TimePeriod'].values\n",
    "            X_pooled.set_index(['index', 'TimePeriod'], inplace=True)\n",
    "\n",
    "            pooled_model = PooledOLS(y_pooled, X_pooled, check_rank=check_rank).fit()\n",
    "            model_attributes.append({'Dataset': key, 'Clusters': use_clusters, 'Model': 'PooledOLS', 'Beta Estimates': pooled_model.params, 'R^2': pooled_model.rsquared})\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    for attributes in model_attributes:\n",
    "        dataset_name = attributes['Dataset']\n",
    "        clusters = attributes['Clusters']\n",
    "        model_name = attributes['Model']\n",
    "        beta_estimates = attributes['Beta Estimates']\n",
    "        r_squared = attributes['R^2']\n",
    "        combined_series = pd.concat([beta_estimates, pd.Series(r_squared, index=[\"$R^2$\"])], axis=0)\n",
    "        result_df = pd.concat([result_df, combined_series], axis=1)\n",
    "        result_df.rename(columns={result_df.columns[-1]: f\"{dataset_name} - {clusters} - {model_name}\"}, inplace=True)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea32bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
