{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0eddc1",
   "metadata": {},
   "source": [
    "### Problem 1: Complete the sentence\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to complete the sentence, \"In the NHST framework, the p-value is the probability of…\"\n",
    "\n",
    "**Given Data:** \n",
    "- The sentence to complete is related to the p-value in the context of Null Hypothesis Significance Testing (NHST).\n",
    "\n",
    "**Explanation:**\n",
    "- In the NHST (Null Hypothesis Significance Testing) framework, the p-value represents the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true.\n",
    "\n",
    "**Interpretation:**\n",
    "- The correct completion of the sentence is:\n",
    "  - **“In the NHST framework, the p-value is the probability of observing data as extreme as, or more extreme than, what was actually observed, under the assumption that the null hypothesis is true.”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f36a3e",
   "metadata": {},
   "source": [
    "### Problem 2: Linear Regression Assumptions\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to outline the assumptions of linear regression, specify a simple linear regression model, and analyze situations where these assumptions might be violated.\n",
    "\n",
    "#### Part A: Write out the four usual assumptions of linear regression.\n",
    "\n",
    "**Given Data:**\n",
    "- Four assumptions of linear regression are required.\n",
    "\n",
    "**Explanation:**\n",
    "- The four usual assumptions of linear regression are:\n",
    "\n",
    "1. **Linearity:** The relationship between the dependent variable y and the independent variable(s) X is linear.\n",
    "2. **Independence:** The residuals (errors) are independent of each other. This means that there is no correlation between the errors for different observations.\n",
    "3. **Homoscedasticity (Constant Variance):** The residuals have constant variance across all levels of the independent variable(s). This implies that the spread of the residuals should be roughly the same at all levels of X.\n",
    "4. **Normality of Residuals:** The residuals of the model are normally distributed. This assumption is important for making valid inferences about the regression coefficients.\n",
    "\n",
    "**Interpretation:**\n",
    "- These assumptions are critical for the validity of hypothesis tests and confidence intervals in linear regression. Violations of these assumptions can lead to biased estimates and incorrect conclusions.\n",
    "\n",
    "#### Part B: Write the complete formal specification of the simple linear regression model.\n",
    "\n",
    "**Formula:**\n",
    "- The formal specification of the simple linear regression model can be written as:\n",
    "  $\n",
    "  y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "  $\n",
    "  where:\n",
    "  - $y_i$ is the dependent variable for observation i,\n",
    "  - $x_i$ is the independent variable for observation i,\n",
    "  - $\\beta_0$ is the intercept of the regression line,\n",
    "  - $\\beta_1$ is the slope of the regression line,\n",
    "  - $\\epsilon_i$ is the error term (or residual) for observation i, which is assumed to be normally distributed with mean 0 and variance $\\sigma^2$ (i.e., $\\epsilon_i$ $\\sim N(0, \\sigma^2)$).\n",
    "\n",
    "**Interpretation:**\n",
    "- This equation represents the expected relationship between the independent variable (x) and the dependent variable (y), with the error term accounting for the deviation from this relationship.\n",
    "\n",
    "#### Part C: Correlated Covariates and Response\n",
    "\n",
    "**Objective:**\n",
    "- Determine if the assumptions of linear regression are still satisfied when covariates are correlated and potentially correlated with the response.\n",
    "\n",
    "**Given Data:**\n",
    "- Two covariates $x_1$ and $x_2$ are correlated, and the response (y) is suspected to be correlated with at least one of the covariates.\n",
    "\n",
    "**Explanation:**\n",
    "- When covariates $x_1$ and $x_2$ are correlated, multicollinearity may arise, which can violate the assumption of independence among predictors. Additionally, if the response (y) is correlated with the covariates, it could lead to issues with the interpretation of the regression coefficients.\n",
    "\n",
    "**Interpretation:**\n",
    "- The assumption of independence among residuals is crucial, and in this case, it might not hold due to the potential multicollinearity. Multicollinearity does not violate the basic assumptions of the linear model but can lead to inflated standard errors and make it difficult to determine the individual effect of each predictor. If multicollinearity is present, it may be necessary to use techniques like Ridge Regression or Principal Component Analysis to address this issue.\n",
    "\n",
    "#### Part D: Example Violating Normality Assumption\n",
    "\n",
    "**Objective:**\n",
    "- Provide a real-world example that would likely violate the normality assumption of residuals.\n",
    "\n",
    "**Example:**\n",
    "- A dataset that records income levels in a population is likely to violate the normality assumption. Income data often follows a skewed distribution, with a large number of individuals earning below the average and a few earning significantly more, leading to a long right tail. This skewness results in residuals that are not normally distributed.\n",
    "\n",
    "**Interpretation:**\n",
    "- Non-normality in residuals can affect the validity of hypothesis tests in the linear regression model. Transformations of the dependent variable or using robust statistical methods can sometimes correct this violation.\n",
    "\n",
    "#### Part E: Example Violating Independence Assumption\n",
    "\n",
    "**Objective:**\n",
    "- Provide a real-world example that would likely violate the independence assumption.\n",
    "\n",
    "**Example:**\n",
    "- Time series data, such as daily stock prices, would likely violate the independence assumption. Observations in time series data are often autocorrelated, meaning that past values influence future values.\n",
    "\n",
    "**Interpretation:**\n",
    "- Violation of independence can lead to underestimated standard errors and incorrect inferences. In such cases, time series models like ARIMA should be used instead of ordinary linear regression.\n",
    "\n",
    "#### Part F: Example Violating Constant Variance Assumption\n",
    "\n",
    "**Objective:**\n",
    "- Provide a real-world example that would likely violate the constant variance assumption.\n",
    "\n",
    "**Example:**\n",
    "- A dataset that records house prices across different neighborhoods might violate the constant variance assumption. In wealthier neighborhoods, the variance in house prices is often larger compared to less affluent areas, leading to heteroscedasticity (non-constant variance).\n",
    "\n",
    "**Interpretation:**\n",
    "- Heteroscedasticity can lead to inefficient estimates and invalid inferences. To address this, weighted least squares or robust standard errors can be used to adjust for non-constant variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28d690",
   "metadata": {},
   "source": [
    "### Problem 3: One Sample Normal Model\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to work with the one-sample normal model, derive maximum likelihood estimates (MLEs), and compute biases.\n",
    "\n",
    "#### Part A: Write down the likelihood for $y_1$, $y_2$, $\\dots$, $y_n$.\n",
    "\n",
    "**Given Data:**\n",
    "- The observations $y_1, y_2, \\dots, y_n$ are assumed to be independent and identically distributed (i.i.d.) following a normal distribution $N(\\mu, \\sigma^2)$.\n",
    "\n",
    "**Formula:**\n",
    "- The likelihood function for a sample of n observations from a normal distribution $N(\\mu, \\sigma^2)$ is given by:\n",
    "  $\n",
    "  L(\\mu, \\sigma^2 | y_1, y_2, \\dots, y_n) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "  $\n",
    "  This is the joint probability of observing the data given the parameters $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "**Interpretation:**\n",
    "- The likelihood function represents the probability of observing the data as a function of the parameters $\\mu and $\\sigma^2$.\n",
    "\n",
    "#### Part B: Derive the MLE of $\\mu$ assuming $\\sigma^2$ is known.\n",
    "\n",
    "**Formula:**\n",
    "- To find the MLE of $\\mu$, we take the natural logarithm of the likelihood function (log-likelihood) and differentiate it with respect to $\\mu$:\n",
    "  \n",
    "  $\n",
    "  \\log L(\\mu | y_1, y_2, \\dots, y_n) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\n",
    "  $\n",
    "  \n",
    "  Differentiating with respect to $\\mu$:\n",
    "  \n",
    "  $\n",
    "  \\frac{\\partial \\log L(\\mu | y_1, y_2, \\dots, y_n)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)\n",
    "  $\n",
    "  \n",
    "  Setting this derivative to zero:\n",
    "  \n",
    "  $\n",
    "  \\sum_{i=1}^{n} (y_i - \\mu) = 0\n",
    "  $\n",
    "  \n",
    "  Solving for $\\mu$:\n",
    "  \n",
    "  $\n",
    "  \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n",
    "  $\n",
    "\n",
    "**Interpretation:**\n",
    "- The MLE for $\\mu$ is the sample mean $\\hat{\\mu}$, which is the most likely value of $\\mu$ given the observed data when $\\sigma^2$ is known.\n",
    "\n",
    "#### Part C: Derive the MLE of $\\sigma^2$ when $\\mu$ is known.\n",
    "\n",
    "**Formula:**\n",
    "- With $\\mu$ known, the log-likelihood function is:\n",
    "  \n",
    "  $\n",
    "  \\log L(\\sigma^2 | y_1, y_2, \\dots, y_n) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\n",
    "  $\n",
    "  \n",
    "  Differentiating with respect to $\\sigma^2$:\n",
    "  \n",
    "  $\n",
    "  \\frac{\\partial \\log L(\\sigma^2 | y_1, y_2, \\dots, y_n)}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (y_i - \\mu)^2\n",
    "  $\n",
    "  \n",
    "  Setting this derivative to zero:\n",
    "  \n",
    "  $\n",
    "  \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\mu)^2\n",
    "  $\n",
    "\n",
    "**Interpretation:**\n",
    "- The MLE for $\\sigma^2$ is the average of the squared deviations from the mean $\\mu$, representing the most likely value of the variance given the observed data.\n",
    "\n",
    "#### Part D: Derive the MLEs for $\\mu$ and $\\sigma^2$ when both are unknown.\n",
    "\n",
    "**Formula:**\n",
    "- The log-likelihood function when both $\\mu$ and $\\sigma^2$ are unknown is:\n",
    "  \n",
    "  $\n",
    "  \\log L(\\mu, \\sigma^2 | y_1, y_2, \\dots, y_n) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\n",
    "  $\n",
    "  \n",
    "  To find the MLEs, differentiate with respect to $\\mu$ and $\\sigma^2$ separately and set the derivatives to zero:\n",
    "  - For $\\mu$:\n",
    "    \n",
    "    $\n",
    "    \\frac{\\partial \\log L(\\mu, \\sigma^2 | y_1, y_2, \\dots, y_n)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) = 0 \\quad \\Rightarrow \\quad \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n",
    "    $\n",
    "    \n",
    "  - For $\\sigma^2$:\n",
    "    \n",
    "    $\n",
    "    \\frac{\\partial \\log L(\\mu, \\sigma^2 | y_1, y_2, \\dots, y_n)}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 \\quad \\Rightarrow \\quad \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\n",
    "    $\n",
    "    \n",
    "\n",
    "**Interpretation:**\n",
    "- The MLEs for $\\mu$ and $\\sigma^2$ are the sample mean $\\hat{\\mu}$ and the sample variance $\\hat{\\sigma}^2$, respectively. These are the most likely estimates for the population parameters given the observed data.\n",
    "\n",
    "#### Part E: Compute the bias for both of the MLEs you calculated in the previous part.\n",
    "\n",
    "**Explanation:**\n",
    "- **Bias of $\\hat{\\mu}$**:\n",
    "  - The sample mean $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$ is an unbiased estimator of $\\mu$, meaning:\n",
    "    \n",
    "    $\n",
    "    \\text{Bias}(\\hat{\\mu}) = E(\\hat{\\mu}) - \\mu = \\mu - \\mu = 0\n",
    "    $\n",
    "    \n",
    "- **Bias of $\\hat{\\sigma}^2$**:\n",
    "  - The sample variance $\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$ is a biased estimator of $\\sigma^2$, with:\n",
    "    \n",
    "    $\n",
    "    \\text{Bias}(\\hat{\\sigma}^2) = E(\\hat{\\sigma}^2) - \\sigma^2 = \\left(\\frac{n-1}{n}\\right)\\sigma^2 - \\sigma^2 = -\\frac{\\sigma^2}{n}\n",
    "    $\n",
    "    \n",
    "    Thus, the bias of $\\hat{\\sigma}^2$ is $-\\frac{\\sigma^2}{n}$.\n",
    "\n",
    "**Interpretation:**\n",
    "- The sample mean $\\hat{\\mu}$ is unbiased, while the sample variance $\\hat{\\sigma}^2$ is biased downward. This bias can be corrected by using $\\frac{1}{n-1}$ instead of $\\frac{1}{n}$ in the calculation of the variance, which leads to the unbiased estimator of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc78e4",
   "metadata": {},
   "source": [
    "### Problem 4: One-Way ANOVA\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to verify an identity in ANOVA, write the null hypothesis, describe key ANOVA terms, and address various aspects related to the ANOVA model.\n",
    "\n",
    "#### Part A: Verify the identity $\\text{SST} = \\text{SSW} + \\text{SSB}$.\n",
    "\n",
    "**Given Data:**\n",
    "- SST: Total Sum of Squares\n",
    "- SSW: Within-group Sum of Squares\n",
    "- SSB: Between-group Sum of Squares\n",
    "\n",
    "**Formula:**\n",
    "- The identity $\\text{SST} = \\text{SSW} + \\text{SSB}$ can be derived as follows:\n",
    "  \n",
    "  1. **Total Sum of Squares (SST):**\n",
    "     $\n",
    "     \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "     $\n",
    "     \n",
    "     where $y_i$ are individual observations and $\\bar{y}$ is the overall mean.\n",
    "\n",
    "  2. **Within-group Sum of Squares (SSW):**\n",
    "     \n",
    "     $\n",
    "     \\text{SSW} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2\n",
    "     $\n",
    "     \n",
    "     where $\\bar{y}_j$ is the mean of group (j) and $( n_j )$ is the number of observations in group j.\n",
    "\n",
    "  3. **Between-group Sum of Squares (SSB):**\n",
    "     $\n",
    "     \\text{SSB} = \\sum_{j=1}^{k} n_j (\\bar{y}_j - \\bar{y})^2\n",
    "     $\n",
    "\n",
    "**Calculation:**\n",
    "- To verify the identity, we express the total variation $( \\text{SST} )$ as the sum of the variation within groups $(\\text{SSW})$ and the variation between groups $(\\text{SSB})$:\n",
    "\n",
    "  $\n",
    "  \\text{SST} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y})^2\n",
    "  $\n",
    "\n",
    "  Decompose $(y_{ij} - \\bar{y})$ into two parts:\n",
    "  $\n",
    "  y_{ij} - \\bar{y} = (y_{ij} - \\bar{y}_j) + (\\bar{y}_j - \\bar{y})\n",
    "  $\n",
    "\n",
    "  Expanding the square:\n",
    "  $\n",
    "  (y_{ij} - \\bar{y})^2 = (y_{ij} - \\bar{y}_j)^2 + 2(y_{ij} - \\bar{y}_j)(\\bar{y}_j - \\bar{y}) + (\\bar{y}_j - \\bar{y})^2\n",
    "  $\n",
    "\n",
    "  Summing over all observations:\n",
    "  $\n",
    "  \\text{SST} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2 + \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (\\bar{y}_j - \\bar{y})^2\n",
    "  $\n",
    "\n",
    "  The cross-product term $(\\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)(\\bar{y}_j - \\bar{y}))$ equals zero because the within-group deviations $(y_{ij} - \\bar{y}_j)$ sum to zero.\n",
    "\n",
    "  Therefore:\n",
    "  $\n",
    "  \\text{SST} = \\text{SSW} + \\text{SSB}\n",
    "  $\n",
    "\n",
    "**Interpretation:**\n",
    "- The identity $(\\text{SST} = \\text{SSW} + \\text{SSB})$ confirms that the total variability in the data is the sum of the variability within groups and the variability between groups. This is a fundamental result in ANOVA that allows partitioning the total variation into components attributable to different sources.\n",
    "\n",
    "#### Part B: Write the null hypothesis for the one-way ANOVA model.\n",
    "\n",
    "**Objective:**\n",
    "- Formulate the null hypothesis for the one-way ANOVA.\n",
    "\n",
    "**Hypothesis:**\n",
    "- The null hypothesis $(H_0)$ for the one-way ANOVA model is:\n",
    "  $\n",
    "  H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k\n",
    "  $\n",
    "  \n",
    "  where $\\mu_j$ represents the mean of the j-th group.\n",
    "\n",
    "**Interpretation:**\n",
    "- The null hypothesis asserts that all group means are equal, meaning that any observed differences among group means are due to random chance rather than systematic effects.\n",
    "\n",
    "#### Part C: Provide a (brief) intuitive description of what SST, SSW, and SSB are measuring.\n",
    "\n",
    "**Explanation:**\n",
    "- **SST (Total Sum of Squares):** \n",
    "  - Measures the total variability in the observed data. It quantifies how much individual observations deviate from the overall mean, providing a measure of the total variation in the dataset.\n",
    "\n",
    "- **SSW (Within-group Sum of Squares):**\n",
    "  - Measures the variability within each group. It quantifies how much the individual observations within each group deviate from their respective group means. This captures the variability that is not explained by the grouping factor.\n",
    "\n",
    "- **SSB (Between-group Sum of Squares):**\n",
    "  - Measures the variability between the group means. It quantifies how much the group means deviate from the overall mean. This component represents the variability that is explained by the differences between the groups.\n",
    "\n",
    "**Interpretation:**\n",
    "- SST represents the overall variation in the data, SSW represents the variation within each group, and SSB represents the variation between groups. The ANOVA test examines whether the between-group variability (SSB) is large enough relative to the within-group variability (SSW) to conclude that the group means are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2074a",
   "metadata": {},
   "source": [
    "### Problem 5: Drug Dosage Effect Analysis\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to determine the appropriate methods or models to analyze the effects of different drug dosages and to construct linear regression models for the data.\n",
    "\n",
    "#### Part A: What method or model would you use to determine if the 50mg dose produces a lower response than the 25mg dose?\n",
    "\n",
    "**Objective:**\n",
    "- Determine if the 50mg dose produces a lower response than the 25mg dose.\n",
    "\n",
    "**Method:**\n",
    "- To determine if the 50mg dose produces a lower response than the 25mg dose, a **paired t-test** or an **independent two-sample t-test** can be used, depending on whether the samples for the 50mg and 25mg doses are paired or independent.\n",
    "\n",
    "**Explanation:**\n",
    "- **Paired t-test**: If the same subjects are given both the 25mg and 50mg doses, then a paired t-test would be appropriate. This test compares the means of the two related groups to determine if there is a statistically significant difference between them.\n",
    "\n",
    "- **Independent two-sample t-test**: If the 25mg and 50mg dose groups are independent (different subjects), an independent two-sample t-test should be used to compare the means of the two groups.\n",
    "\n",
    "**Formula for Independent t-test:**\n",
    "$\n",
    "t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $(\\bar{x}_1)$ and $(\\bar{x}_2)$ are the sample means for the 25mg and 50mg doses, respectively.\n",
    "- $(s_1^2)$ and $(s_2^2)$ are the sample variances for the two groups.\n",
    "- $(n_1)$ and $(n_2)$ are the sample sizes for the two groups.\n",
    "\n",
    "**Interpretation:**\n",
    "- A significant negative t-value (with a corresponding low p-value) would suggest that the 50mg dose produces a lower response than the 25mg dose.\n",
    "\n",
    "#### Part B: What method or model would you use to determine if the effect of the dose with the highest average response value has a significantly greater effect relative to the other dosages?\n",
    "\n",
    "**Objective:**\n",
    "- Determine if the dose with the highest average response has a significantly greater effect than the other dosages.\n",
    "\n",
    "**Method:**\n",
    "- **One-way ANOVA** followed by **post hoc tests** (e.g., Tukey's HSD) would be appropriate to compare the mean responses across all dosage levels and determine if the highest response is significantly greater than the others.\n",
    "\n",
    "**Explanation:**\n",
    "- **One-way ANOVA**: This test compares the means across multiple groups (dosages in this case) to see if at least one mean is significantly different from the others.\n",
    "- **Post hoc tests**: If the ANOVA is significant, post hoc tests like Tukey's HSD (Honestly Significant Difference) are conducted to determine which specific groups (dosages) differ from each other.\n",
    "\n",
    "**Formula for ANOVA F-statistic:**\n",
    "$\n",
    "F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- **MSB (Mean Square Between)** represents the variance between the group means.\n",
    "- **MSW (Mean Square Within)** represents the variance within the groups.\n",
    "\n",
    "**Interpretation:**\n",
    "- If the ANOVA shows a significant F-statistic, and the post hoc tests reveal that the dose with the highest average response is significantly greater than the others, then it can be concluded that this dose has a significantly greater effect.\n",
    "\n",
    "#### Part C: Construct a linear regression model for the sample, assuming nothing is known regarding the relationship of the average response values at the different dosages.\n",
    "\n",
    "**Objective:**\n",
    "- Construct a basic linear regression model for the sample data.\n",
    "\n",
    "**Model:**\n",
    "- A simple linear regression model can be constructed as follows:\n",
    "$\n",
    "y_i = \\beta_0 + \\beta_1 \\text{Dose}_{i} + \\epsilon_i\n",
    "$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the response variable for observation i.\n",
    "- $\\text{Dose}_i$ represents the dosage level (e.g., 10mg, 25mg, 50mg).\n",
    "- $\\beta_0$ is the intercept.\n",
    "- $\\beta_1$ is the coefficient for the dosage level.\n",
    "- $\\epsilon_i$ is the error term.\n",
    "\n",
    "**Interpretation:**\n",
    "- This model assumes a linear relationship between the dosage and the response. The coefficient $\\beta_1$ indicates the expected change in the response for each unit increase in the dosage.\n",
    "\n",
    "#### Part D: Construct an alternative linear regression model for the sample, now assuming that the average response is quadratic with respect to the dosages.\n",
    "\n",
    "**Objective:**\n",
    "- Construct a quadratic regression model to capture a potential nonlinear relationship.\n",
    "\n",
    "**Model:**\n",
    "- The quadratic regression model is constructed as:\n",
    "$\n",
    "y_i = \\beta_0 + \\beta_1 \\text{Dose}_{i} + \\beta_2 \\text{Dose}_{i}^2 + \\epsilon_i\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\text{Dose}_{i}^2$ is the square of the dosage level.\n",
    "\n",
    "**Interpretation:**\n",
    "- This model accounts for the possibility that the relationship between dosage and response is not purely linear but may have a quadratic (curved) relationship. The coefficient $\\beta_2$ captures the curvature, indicating how the effect of dosage changes as the dosage level increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b811e57",
   "metadata": {},
   "source": [
    "### Problem 6: Simple Linear Regression (SLR)\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to write the likelihood function for simple linear regression, and to derive the maximum likelihood estimate (MLE) for the coefficient, assuming the error variance is known.\n",
    "\n",
    "#### Part A: Write the likelihood for SLR.\n",
    "\n",
    "**Given Data:**\n",
    "- Simple Linear Regression model: $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, where $\\epsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "**Formula:**\n",
    "- The likelihood function for a simple linear regression model is derived based on the assumption that the errors $(\\epsilon_i) are normally distributed with mean 0 and variance $(\\sigma^2)$.\n",
    "\n",
    "  For n observations, the likelihood function is given by:\n",
    "  $\n",
    "  L(\\beta_0, \\beta_1, \\sigma^2 | y_1, y_2, \\dots, y_n) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\\right)\n",
    "  $\n",
    "  \n",
    "  - $y_i$ is the observed value,\n",
    "  - $x_i$ is the value of the independent variable,\n",
    "  - $\\beta_0$ and $\\beta_1$ are the intercept and slope of the regression line, respectively,\n",
    "  - $\\sigma^2$ is the error variance.\n",
    "\n",
    "**Interpretation:**\n",
    "- The likelihood function represents the probability of observing the given data $(y_1, y_2, \\dots, y_n)$ as a function of the parameters $\\beta_0$, $\\beta_1$, and $\\sigma^2$.\n",
    "\n",
    "#### Part B: Derive the MLE for the coefficient $\\beta_1$, assuming the error variance is known.\n",
    "\n",
    "**Objective:**\n",
    "- Derive the MLE for $\\beta_1$ assuming that the error variance $\\sigma^2$ is known.\n",
    "\n",
    "**Formula:**\n",
    "- First, take the natural logarithm of the likelihood function to obtain the log-likelihood function:\n",
    "  $\n",
    "  \\log L(\\beta_0, \\beta_1 | y_1, y_2, \\dots, y_n) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "  $\n",
    "\n",
    "- To find the MLE for $\\beta_1$, differentiate the log-likelihood function with respect to $\\beta_1$ and set the derivative equal to zero:\n",
    "\n",
    "  $\n",
    "  \\frac{\\partial \\log L(\\beta_1)}{\\partial \\beta_1} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n",
    "  $\n",
    "\n",
    "- Simplifying this equation:\n",
    "  $\n",
    "  \\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} x_i \\beta_0 - \\beta_1 \\sum_{i=1}^{n} x_i^2 = 0\n",
    "  $\n",
    "\n",
    "- Solve for $\\beta_1$:\n",
    "  $\n",
    "  \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} x_i y_i - \\hat{\\beta}_0 \\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} x_i^2}\n",
    "  $\n",
    "\n",
    "  However, since $\\hat{\\beta}_0$ can be estimated separately by considering the intercept, we usually express $\\hat{\\beta}_1$ as:\n",
    "  $\n",
    "  \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "  $\n",
    "  \n",
    "  where:\n",
    "  - $\\bar{x}$ and $\\bar{y}$ are the sample means of $x_i$ and $y_i$, respectively.\n",
    "\n",
    "**Interpretation:**\n",
    "- The MLE for $\\beta_1$ is the slope of the regression line that minimizes the sum of squared differences between the observed values $y_i$ and the values predicted by the model. This estimate is unbiased and consistent under the assumption that the errors $\\epsilon_i$ are normally distributed with mean 0 and variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67089d9f",
   "metadata": {},
   "source": [
    "### Problem 7: Impact of Adding Irrelevant Covariates on $R^2$\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to explain why adding irrelevant covariates (i.e., covariates that are not associated with the response) to a linear regression model will strictly increase the $R^2$ measure.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "**Given Data:**\n",
    "- Linear regression model with $R^2$ as a measure of goodness-of-fit.\n",
    "\n",
    "**Concept:**\n",
    "- $R^2$, or the coefficient of determination, is a measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variables in the model. It is calculated as:\n",
    "  $\n",
    "  R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\n",
    "  $\n",
    "  \n",
    "  where:\n",
    "  - $\\text{SS}_{\\text{res}}$ is the residual sum of squares (unexplained variation),\n",
    "  - $\\text{SS}_{\\text{tot}}$ is the total sum of squares (total variation in the dependent variable).\n",
    "\n",
    "**Explanation:**\n",
    "- **Adding Irrelevant Covariates:**\n",
    "  - When an irrelevant covariate (a variable not associated with the response) is added to the model, it does not help explain any additional variance in the dependent variable. However, because the model now has more parameters, the residual sum of squares $(\\text{SS}_{\\text{res}})$ typically decreases or stays the same.\n",
    "  \n",
    "  - Even though the irrelevant covariate does not contribute meaningful information, the mathematical structure of the least squares estimation process can still find a coefficient for it that minimizes the residual sum of squares. This is because the $R^2$ measure is calculated purely based on the reduction in $\\text{SS}_{\\text{res}}$, regardless of whether the reduction is statistically meaningful.\n",
    "\n",
    "- **Impact on $R^2$:**\n",
    "  - $R^2$ is a non-decreasing function of the number of covariates in the model. This means that adding any new covariate, relevant or not, will either increase $R^2$ or leave it unchanged, but it will never decrease it.\n",
    "  \n",
    "  - Therefore, even if the added covariate is irrelevant (i.e., its true coefficient is zero), the model's $R^2$ can still increase because the additional parameter allows the model to fit the data more closely, even if only due to random fluctuations.\n",
    "\n",
    "**Interpretation:**\n",
    "- The increase in $R^2$ from adding irrelevant covariates does not imply that the model has improved in a meaningful way. Instead, it reflects a mathematical artifact of how $R^2$ is calculated. To prevent overfitting and to ensure that added covariates are truly useful, adjusted $R^2$ or other model selection criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) should be used, which penalize the addition of unnecessary covariates.\n",
    "\n",
    "**Adjusted $R^2$:**\n",
    "- Adjusted $R^2$ is defined as:\n",
    "  $\n",
    "  \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1-R^2)(n-1)}{n-p-1} \\right)\n",
    "  $\n",
    "  \n",
    "  where n is the number of observations, and p is the number of predictors. This version of $R^2$ takes into account the number of predictors in the model and only increases if the added covariate improves the model sufficiently to justify the increase in complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7823ab9",
   "metadata": {},
   "source": [
    "### Problem 8: Estimating the Average Growth Rate of Bacteria\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to propose a method for estimating the average growth rate of bacteria given population counts at evenly spaced intervals and to critique a method for averaging growth rates across multiple samples.\n",
    "\n",
    "#### Part A: The Principal Investigator (PI) asks you to produce a confidence interval for the average growth rate. The data consists of population counts for one culture, recorded at evenly spaced intervals, which can be assumed to be scaled to [0, 1]. Propose an appropriate method or model.\n",
    "\n",
    "**Given Data:**\n",
    "- Population counts for one culture recorded at evenly spaced intervals.\n",
    "- Growth is known to be exponential with an unknown rate.\n",
    "\n",
    "**Method:**\n",
    "- The population growth of bacteria can be modeled using an exponential growth model. The exponential growth equation is:\n",
    "  $\n",
    "  y(t) = y_0 \\exp(rt)\n",
    "  $\n",
    "  \n",
    "  where:\n",
    "  - $y(t)$ is the population size at time t,\n",
    "  - $y_0$ is the initial population size,\n",
    "  - r is the growth rate.\n",
    "\n",
    "- Taking the natural logarithm of both sides:\n",
    "  $\n",
    "  \\log y(t) = \\log y_0 + rt\n",
    "  $\n",
    "  \n",
    "  This is a linear relationship where $\\log y(t)$ is the dependent variable, t is the independent variable, $\\log y_0$ is the intercept, and r is the slope (growth rate).\n",
    "\n",
    "**Steps:**\n",
    "1. **Fit a Linear Model:**\n",
    "   - Fit a simple linear regression model using $\\log y(t)$ as the response variable and t as the predictor.\n",
    "   - The slope of the fitted line will be an estimate of the growth rate r.\n",
    "\n",
    "2. **Estimate the Growth Rate:**\n",
    "   - The estimated growth rate $\\hat{r}$ is the slope from the linear regression.\n",
    "\n",
    "3. **Construct a Confidence Interval for $\\hat{r}$:**\n",
    "   - Assuming the residuals from the linear regression are normally distributed, construct a confidence interval for the growth rate (r) using the standard error of the slope.\n",
    "   - The confidence interval can be calculated as:\n",
    "     $\n",
    "     \\hat{r} \\pm t_{\\alpha/2, n-2} \\cdot \\text{SE}(\\hat{r})\n",
    "     $\n",
    "     \n",
    "     where $(t_{\\alpha/2, n-2}$ is the critical value from the t-distribution with ( n-2 ) degrees of freedom, and $\\text{SE}(\\hat{r})$ is the standard error of the estimated growth rate.\n",
    "\n",
    "**Interpretation:**\n",
    "- This method provides a statistically valid confidence interval for the average growth rate r of the bacteria population based on the observed data.\n",
    "\n",
    "#### Part B: The PI now has data from 4 additional cultures. They ask you to repeat the analysis you did in the previous part on each of these new samples so that the average growth rates can be averaged together to provide a better estimate of the population growth rate. Explain one or two significant flaws in this method and propose a better one.\n",
    "\n",
    "**Objective:**\n",
    "- Critique the method of simply averaging the growth rates from multiple samples and propose a better method.\n",
    "\n",
    "**Flaws in Averaging the Growth Rates:**\n",
    "1. **Ignoring Variability:**\n",
    "   - Simply averaging the estimated growth rates from multiple samples ignores the variability in the estimates. Different cultures might have different growth conditions or measurement errors, leading to growth rates with different levels of uncertainty.\n",
    "   - Averaging the rates without considering their variance might give undue weight to less reliable estimates.\n",
    "\n",
    "2. **Assumption of Independence:**\n",
    "   - This method assumes that the growth rates are independent across samples. However, if there are shared environmental factors or systematic biases affecting all cultures, this assumption might not hold, leading to misleading results.\n",
    "\n",
    "**Proposed Better Method:**\n",
    "- **Use a Hierarchical (Random Effects) Model:**\n",
    "  - A better approach would be to use a hierarchical model that accounts for the variability between cultures. This model treats the growth rate as a random effect that varies across cultures.\n",
    "\n",
    "**Steps:**\n",
    "1. **Model Setup:**\n",
    "   - Assume the growth rate $(r_j)$ for the j-th culture is drawn from a normal distribution:\n",
    "     $\n",
    "     r_j \\sim N(\\mu_r, \\tau^2)\n",
    "     $\n",
    "     \n",
    "     where $\\mu_r$ is the overall mean growth rate, and $\\tau^2$ is the between-culture variance.\n",
    "\n",
    "2. **Estimation:**\n",
    "   - Fit the hierarchical model using all the data from the 5 cultures. The overall mean $\\mu_r$ will be estimated, taking into account both the within-culture variability and the between-culture variability.\n",
    "\n",
    "3. **Construct a Confidence Interval:**\n",
    "   - Construct a confidence interval for $\\mu_r$, which represents the population-level average growth rate.\n",
    "\n",
    "**Interpretation:**\n",
    "- This method provides a more accurate estimate of the population growth rate by accounting for variability between different cultures. It also provides a more reliable confidence interval by incorporating the uncertainty from multiple sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5c023",
   "metadata": {},
   "source": [
    "### Problem 9: Two-Sample t-Test and ANOVA\n",
    "\n",
    "**Objective:** \n",
    "- The objective is to show the equivalence between the two-sample t-test and ANOVA, and to demonstrate how a regression model can be used to conduct an equivalent test.\n",
    "\n",
    "#### Part A: Show that the two-sample t-test and ANOVA are identical.\n",
    "\n",
    "**Given Data:**\n",
    "- We have two groups, with observations from each group assumed to be normally distributed with equal variances.\n",
    "\n",
    "**Two-Sample t-Test:**\n",
    "- The two-sample t-test compares the means of two independent groups to determine if they are significantly different.\n",
    "- The test statistic for the two-sample t-test is:\n",
    "  $\n",
    "  t = \\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{s_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n",
    "  $\n",
    "  \n",
    "  where:\n",
    "  - $\\bar{y}_1$ and $\\bar{y}_2$ are the sample means for the two groups,\n",
    "  - $n_1$ and $n_2$ are the sample sizes for the two groups,\n",
    "  - $s_p^2$ is the pooled variance, calculated as:\n",
    "    $\n",
    "    s_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n",
    "    $\n",
    "    \n",
    "  - $s_1^2$ and $s_2^2$ are the sample variances for the two groups.\n",
    "\n",
    "**One-Way ANOVA:**\n",
    "- One-way ANOVA compares the means across multiple groups (including just two groups) by partitioning the total variation into variation between groups (SSB) and within groups (SSW).\n",
    "- The F-statistic in ANOVA is given by:\n",
    "  $\n",
    "  F = \\frac{\\text{SSB}/\\text{df}_{\\text{between}}}{\\text{SSW}/\\text{df}_{\\text{within}}}\n",
    "  $\n",
    "  \n",
    "  where:\n",
    "  - $\\text{SSB}$ is the sum of squares between the groups,\n",
    "  - $\\text{SSW}$ is the sum of squares within the groups,\n",
    "  - $\\text{df}_{\\text{between}}$ = k - 1 (for two groups, (k = 2)),\n",
    "  - $\\text{df}_{\\text{within}} = n_1 + n_2 - 2$.\n",
    "\n",
    "**Equivalence:**\n",
    "- When there are only two groups, the F-statistic from ANOVA is directly related to the square of the t-statistic from the two-sample t-test. Specifically:\n",
    "  $\n",
    "  F = t^2\n",
    "  $\n",
    "  \n",
    "- This shows that the F-statistic from a one-way ANOVA with two groups will yield the same result as the square of the t-statistic from a two-sample t-test. Therefore, the two tests are equivalent in this case.\n",
    "\n",
    "**Interpretation:**\n",
    "- The equivalence of the t-test and ANOVA for two groups means that either test can be used to determine if the means of two groups are significantly different. They both rely on the same underlying assumptions and will produce the same p-value.\n",
    "\n",
    "#### Part B: Show that the regression model $y_{ij} = \\beta_0 + \\beta_1 I(j = 1) + \\epsilon_{ij}$ can also be used to conduct an equivalent test as the two-sample t-test and ANOVA.\n",
    "\n",
    "**Given Data:**\n",
    "- $y_{ij}$ represents the response for the i-th observation in the j-th group.\n",
    "- I(j = 1) is an indicator variable that takes the value 1 if the observation is in group 1, and 0 otherwise.\n",
    "\n",
    "**Regression Model:**\n",
    "- The model can be written as:\n",
    "  $\n",
    "  y_{ij} = \\beta_0 + \\beta_1 I(j = 1) + \\epsilon_{ij}\n",
    "  $\n",
    "  \n",
    "  where:\n",
    "  - $\\beta_0$ represents the mean of the group when I(j = 1) = 0 (i.e., the second group),\n",
    "  - $\\beta_1$ represents the difference in means between the two groups,\n",
    "  - $\\epsilon_{ij}$ is the error term assumed to be normally distributed with mean 0 and constant variance.\n",
    "\n",
    "**Equivalence to t-Test:**\n",
    "- The hypothesis $H_0: \\beta_1$ = 0 in this regression model tests whether there is a significant difference in means between the two groups. This is equivalent to the hypothesis tested in the two-sample t-test and one-way ANOVA.\n",
    "- The t-statistic for $\\beta_1$ in the regression model is calculated as:\n",
    "  $\n",
    "  t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}\n",
    "  $\n",
    "  \n",
    "  where $\\text{SE}(\\hat{\\beta}_1)$ is the standard error of the estimated coefficient $\\hat{\\beta}_1$.\n",
    "\n",
    "**Equivalence to ANOVA:**\n",
    "- The sum of squares explained by the regression model (i.e., due to $\\beta_1$) corresponds to the sum of squares between groups (SSB) in ANOVA.\n",
    "- The residual sum of squares in the regression model corresponds to the sum of squares within groups (SSW) in ANOVA.\n",
    "- The F-statistic for the regression model is the square of the t-statistic for $\\beta_1$, just as in the equivalence between the two-sample t-test and ANOVA.\n",
    "\n",
    "**Interpretation:**\n",
    "- The regression model $y_{ij} = \\beta_0 + \\beta_1 I(j = 1) + \\epsilon_{ij}$ provides an equivalent test to the two-sample t-test and one-way ANOVA. This demonstrates that the same statistical test can be framed in multiple ways, each of which provides the same result when the underlying assumptions are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1066519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
