{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df316a7d",
   "metadata": {},
   "source": [
    "### Problem 1: Intercept-Free Simple Linear Regression\n",
    "\n",
    "**Objective:**  \n",
    "To compute the Maximum Likelihood Estimator (MLE) for the slope parameter $\\beta$ in an intercept-free simple linear regression model and to understand the differences in interpretation between this model and the usual simple linear regression model with an intercept.\n",
    "\n",
    "**Given Data:**  \n",
    "- The intercept-free simple linear regression model:  \n",
    "  $\n",
    "  y_i = \\beta x_i + \\epsilon_i\n",
    "  $\n",
    "  where $\\epsilon_i$ are independent and identically distributed as $N(0, \\sigma^2)$.\n",
    "\n",
    "- $\\sigma^2$ is known.\n",
    "\n",
    "**Formula:**  \n",
    "The log-likelihood function for the given data can be written as:\n",
    "$\n",
    "\\ell(\\beta) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left( y_i - \\beta x_i \\right)^2\n",
    "$\n",
    "\n",
    "**Calculation:**  \n",
    "1. Differentiate the log-likelihood function with respect to $\\beta$:\n",
    "   $\n",
    "   \\frac{d\\ell(\\beta)}{d\\beta} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left( y_i - \\beta x_i \\right) x_i\n",
    "   $\n",
    "   \n",
    "2. Set the derivative equal to zero to find the maximum:\n",
    "   $\n",
    "   \\sum_{i=1}^{n} y_i x_i - \\beta \\sum_{i=1}^{n} x_i^2 = 0\n",
    "   $\n",
    "   \n",
    "   $\n",
    "   \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} y_i x_i}{\\sum_{i=1}^{n} x_i^2}\n",
    "   $\n",
    "   \n",
    "   Hence, the MLE for $\\beta$ is:\n",
    "   $\n",
    "   \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} y_i x_i}{\\sum_{i=1}^{n} x_i^2}\n",
    "   $\n",
    "\n",
    "**Interpretation:**  \n",
    "- In the usual simple linear regression model, $\\beta_1$ represents the change in y for a one-unit change in x, assuming the intercept $\\beta_0$ captures the baseline value of y when x = 0.\n",
    "- In the intercept-free model, $\\beta$ directly represents the change in y for a one-unit change in x, but this model assumes that the regression line passes through the origin (i.e., y = 0 when x = 0). Therefore, the intercept-free model is particularly useful when it is known a priori that the line must pass through the origin.\n",
    "\n",
    "**Advantages of the Intercept-Free Model:**  \n",
    "- **Efficiency:** The intercept-free model has fewer parameters to estimate, which can lead to more efficient estimates of $\\beta$ when the true relationship between y and x is known to pass through the origin.\n",
    "- **Interpretation:** When there is theoretical or empirical evidence that the relationship between y and x passes through the origin, the intercept-free model provides a more accurate representation of this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd086ed7",
   "metadata": {},
   "source": [
    "### Problem 2: Significance of $\\beta_1$ in Multiple Regression\n",
    "\n",
    "**Objective:**  \n",
    "To determine whether the significance of $\\beta_1$ in a simple linear regression model implies its significance in a multiple regression model when an additional covariate $x_2$ is added.\n",
    "\n",
    "**Given Data:**  \n",
    "- Two models are provided:\n",
    "  1. Simple Linear Regression (SLR) model:  \n",
    "     $\n",
    "     y_i = \\beta_0 + \\beta_1 x_{i1} + \\epsilon_i\n",
    "     $\n",
    "     \n",
    "  2. Multiple Linear Regression (MLR) model:  \n",
    "     $\n",
    "     y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\n",
    "     $\n",
    "\n",
    "**Formula:**  \n",
    "- The significance of a coefficient in a regression model is typically assessed using a t-test, which checks whether the coefficient is significantly different from zero.\n",
    "- In the SLR model, the t-statistic for $\\beta_1$ is calculated as:\n",
    "  $\n",
    "  t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}\n",
    "  $\n",
    "  \n",
    "- The same procedure applies in the MLR model, but the inclusion of an additional variable $x_2$ affects the estimate $\\hat{\\beta}_1$ and its standard error $\\text{SE}(\\hat{\\beta}_1)$.\n",
    "\n",
    "**Explanation:**  \n",
    "1. **Significance in SLR:**  \n",
    "   When $\\beta_1$ is found to be significant in the SLR model, it implies that $x_1$ has a statistically significant linear relationship with y when considered alone.\n",
    "   \n",
    "2. **Significance in MLR:**  \n",
    "   When an additional variable $x_2$ is added to the model, the significance of $\\beta_1$ may change due to the following reasons:\n",
    "   - **Multicollinearity:** If $x_1$ and $x_2$ are highly correlated, the standard error of $\\hat{\\beta}_1$ may increase, leading to a decrease in the t-statistic, which could make $\\beta_1$ non-significant.\n",
    "   - **Confounding or Interaction:** $x_2$ might capture some of the effect that was attributed to $x_1$ in the SLR model, or there could be an interaction between $x_1$ and $x_2$ that was not accounted for in the SLR model.\n",
    "   - **Model Specification:** The additional variable might change the relationship structure between $x_1$ and y, which could either enhance or diminish the significance of $\\beta_1$.\n",
    "\n",
    "**Conclusion:**  \n",
    "- The significance of $\\beta_1$ in the SLR model does not guarantee its significance in the MLR model. The significance can change based on the relationship between $x_1$ and $x_2$, and how they both relate to y.\n",
    "- In practice, when adding variables to a regression model, it's important to check the significance of all coefficients again, as the dynamics between predictors may lead to different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3673c25",
   "metadata": {},
   "source": [
    "### Problem 3: Estimation in Simple Linear Regression with Known Intercept\n",
    "\n",
    "**Objective:**  \n",
    "To find the MLEs for $\\beta_1$ and $\\sigma^2$ in a simple linear regression model where the intercept $\\beta_0$ is known, and to compute the bias of these estimators.\n",
    "\n",
    "**Given Data:**  \n",
    "- Simple Linear Regression (SLR) model with known intercept $\\beta_0$:\n",
    "  $\n",
    "  y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "  $\n",
    "  \n",
    "  where $\\epsilon_i$ are independent and identically distributed as $N(0, \\sigma^2)$.\n",
    "\n",
    "**Formula:**  \n",
    "- The log-likelihood function for the given data is:\n",
    "  $\n",
    "  \\ell(\\beta_1, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\beta_1 x_i \\right)^2\n",
    "  $\n",
    "  \n",
    "- To find the MLEs, differentiate the log-likelihood function with respect to $\\beta_1$ and $\\sigma^2$ and set the derivatives equal to zero.\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "**a.) Finding MLE for $\\beta_1$ and $\\sigma^2$:**\n",
    "\n",
    "1. **MLE for $\\beta_1$:**\n",
    "   - Differentiate the log-likelihood function with respect to $\\beta_1$:\n",
    "     $\n",
    "     \\frac{d\\ell(\\beta_1)}{d\\beta_1} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\beta_1 x_i \\right) x_i\n",
    "     $\n",
    "     \n",
    "   - Set the derivative equal to zero:\n",
    "     $\n",
    "     \\sum_{i=1}^{n} (y_i - \\beta_0) x_i = \\beta_1 \\sum_{i=1}^{n} x_i^2\n",
    "     $\n",
    "     \n",
    "     $\n",
    "     \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (y_i - \\beta_0) x_i}{\\sum_{i=1}^{n} x_i^2}\n",
    "     $\n",
    "   \n",
    "2. **MLE for $\\sigma^2$:**\n",
    "   - Substitute $\\hat{\\beta}_1$ back into the log-likelihood and differentiate with respect to $\\sigma^2$:\n",
    "     $\n",
    "     \\frac{d\\ell(\\sigma^2)}{d\\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\hat{\\beta}_1 x_i \\right)^2\n",
    "     $\n",
    "     \n",
    "   - Set the derivative equal to zero:\n",
    "     $\n",
    "     \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\hat{\\beta}_1 x_i \\right)^2\n",
    "     $\n",
    "\n",
    "**b.) Computing Bias of MLEs:**\n",
    "\n",
    "1. **Bias of $\\hat{\\beta}_1$:**\n",
    "   - The estimator $\\hat{\\beta}_1$ is unbiased if $E[\\hat{\\beta}_1]$ = $\\beta_1$.\n",
    "   - Since $\\hat{\\beta}_1$ is a linear combination of $y_i$, and $E[y_i] = \\beta_0 + \\beta_1 x_i$, the expectation is:\n",
    "     $\n",
    "     E[\\hat{\\beta}_1] = \\frac{\\sum_{i=1}^{n} E[(y_i - \\beta_0) x_i]}{\\sum_{i=1}^{n} x_i^2} = \\beta_1\n",
    "     $\n",
    "     \n",
    "   - Therefore, $\\hat{\\beta}_1$ is an unbiased estimator of $\\beta_1$.\n",
    "\n",
    "2. **Bias of $\\hat{\\sigma}^2$:**\n",
    "   - The estimator $\\hat{\\sigma}^2$ is biased because:\n",
    "     $\n",
    "     E[\\hat{\\sigma}^2] = \\frac{1}{n} E\\left[\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\hat{\\beta}_1 x_i \\right)^2\\right]\n",
    "     $\n",
    "     \n",
    "   - After simplifying, we find that:\n",
    "     $\n",
    "     E[\\hat{\\sigma}^2] = \\sigma^2 \\left(1 - \\frac{1}{n}\\right)\n",
    "     $\n",
    "     \n",
    "   - Therefore, $\\hat{\\sigma}^2$ is a biased estimator of $\\sigma^2$, with a bias equal to $-\\frac{\\sigma^2}{n}$.\n",
    "\n",
    "**Conclusion:**  \n",
    "- The MLE for $\\beta_1$ is $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (y_i - \\beta_0) x_i}{\\sum_{i=1}^{n} x_i^2}$, and it is an unbiased estimator.\n",
    "- The MLE for $\\sigma^2$ is $\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\hat{\\beta}_1 x_i \\right)^2$, but it is biased with a bias of $-\\frac{\\sigma^2}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67fadd",
   "metadata": {},
   "source": [
    "### Problem 4: Comparison Between Standard Normal Distribution and t-Distribution\n",
    "\n",
    "**Objective:**  \n",
    "To explain the differences between a standard normal distribution and a t-distribution and to determine whether a 95% confidence interval from a t-distribution will be wider or narrower compared to one from a standard normal distribution.\n",
    "\n",
    "**Given Data:**  \n",
    "- The standard normal distribution is denoted as N(0, 1).\n",
    "- The t-distribution is a family of distributions, indexed by degrees of freedom (df), and is denoted as $t_{\\nu}$, where $\\nu$ is the degrees of freedom.\n",
    "\n",
    "**Formula:**  \n",
    "- A 95% confidence interval for a parameter (e.g., mean) using a standard normal distribution is given by:\n",
    "  $\n",
    "  \\text{CI} = \\hat{\\theta} \\pm z_{\\alpha/2} \\cdot \\text{SE}(\\hat{\\theta})\n",
    "  $\n",
    "  \n",
    "  where $z_{\\alpha/2}$ is the critical value from the standard normal distribution.\n",
    "  \n",
    "- A 95% confidence interval using a t-distribution is given by:\n",
    "  $\n",
    "  \\text{CI} = \\hat{\\theta} \\pm t_{\\nu,\\alpha/2} \\cdot \\text{SE}(\\hat{\\theta})\n",
    "  $\n",
    "  \n",
    "  where $t_{\\nu,\\alpha/2}$ is the critical value from the t-distribution with $\\nu$ degrees of freedom.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "1. **Standard Normal Distribution (N(0, 1)):**\n",
    "   - The standard normal distribution is symmetric and bell-shaped, centered at 0 with a standard deviation of 1.\n",
    "   - It assumes that the population variance is known and that the sample size is large enough for the central limit theorem to apply, making the sample mean approximately normally distributed.\n",
    "\n",
    "2. **t-Distribution:**\n",
    "   - The t-distribution is also symmetric and bell-shaped but has heavier tails than the standard normal distribution. This means it has more probability in the tails, making it more robust to outliers.\n",
    "   - The t-distribution accounts for the uncertainty in estimating the population standard deviation from a sample, particularly when the sample size is small. The degrees of freedom $\\nu$ = n - 1 where n is the sample size.\n",
    "   - As the degrees of freedom increase (i.e., as the sample size increases), the t-distribution approaches the standard normal distribution.\n",
    "\n",
    "3. **Comparison of Confidence Intervals:**\n",
    "   - A 95% confidence interval using the t-distribution will be wider than one using the standard normal distribution, especially for small sample sizes. This is because the t-distribution accounts for the additional variability due to estimating the population standard deviation from the sample.\n",
    "   - The wider interval reflects the greater uncertainty in the estimate of the mean when using the t-distribution with fewer degrees of freedom.\n",
    "\n",
    "**Conclusion:**  \n",
    "- The main difference between the standard normal distribution and the t-distribution lies in the heavier tails of the t-distribution, which provide a more conservative estimate when the sample size is small.\n",
    "- A 95% confidence interval computed from a t-distribution will generally be wider than one computed from a standard normal distribution, particularly when the sample size is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfa9f0",
   "metadata": {},
   "source": [
    "### Problem 5: Multiple Linear Regression with an Orthogonal Design Matrix\n",
    "\n",
    "**Objective:**  \n",
    "To simplify the expressions for the estimated coefficients $\\hat{\\beta}$ and the predicted values $\\hat{Y}$ in a multiple linear regression model when the design matrix X is orthogonal, and to find the variance of $\\hat{\\beta}$.\n",
    "\n",
    "**Given Data:**  \n",
    "- The multiple linear regression model is given by:\n",
    "  $\n",
    "  Y = X\\beta + \\epsilon\n",
    "  $\n",
    "  \n",
    "  where $\\epsilon \\sim N(0, \\sigma^2 I_n)$ and X is the design matrix.\n",
    "  \n",
    "- The design matrix X is orthogonal, meaning $X^T X = I_p$, where $I_p$ is the identity matrix of size $p \\times p$.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "1. **Estimating $\\hat{\\beta}$:**\n",
    "   - The ordinary least squares (OLS) estimator for $\\beta$ is:\n",
    "     $\n",
    "     \\hat{\\beta} = (X^T X)^{-1} X^T Y\n",
    "     $\n",
    "\n",
    "2. **Predicted values $\\hat{Y}$:**\n",
    "   - The predicted values of the response variable are given by:\n",
    "     $\n",
    "     \\hat{Y} = X\\hat{\\beta}\n",
    "     $\n",
    "\n",
    "3. **Variance of $\\hat{\\beta}$:**\n",
    "   - The variance-covariance matrix of $\\hat{\\beta}$ is given by:\n",
    "     $\n",
    "     \\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n",
    "     $\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "**a.) Simplify $\\hat{\\beta}$ and $\\hat{Y}$ for Orthogonal Design Matrix:**\n",
    "\n",
    "1. **Simplifying $\\hat{\\beta}$:**\n",
    "   - Given that $X^T X = I_p$, the expression for $\\hat{\\beta}$ simplifies to:\n",
    "     $\n",
    "     \\hat{\\beta} = (I_p)^{-1} X^T Y = X^T Y\n",
    "     $\n",
    "     \n",
    "   - Therefore, when X is orthogonal, the OLS estimator for $\\beta$ is simply:\n",
    "     $\n",
    "     \\hat{\\beta} = X^T Y\n",
    "     $\n",
    "\n",
    "2. **Simplifying $\\hat{Y}$:**\n",
    "   - The predicted values $\\hat{Y}$ are given by:\n",
    "     $\n",
    "     \\hat{Y} = X \\hat{\\beta} = X X^T Y\n",
    "     $\n",
    "     \n",
    "   - Since $X^T X = I_p$, the product $X X^T$ is an identity matrix, which means:\n",
    "     $\n",
    "     \\hat{Y} = Y\n",
    "     $\n",
    "   - This indicates that the fitted values $\\hat{Y}$ equal the observed values Y when the design matrix is orthogonal.\n",
    "\n",
    "**b.) Variance of $\\hat{\\beta}$:**\n",
    "\n",
    "- Using the formula for the variance of $\\hat{\\beta}$:\n",
    "  $\n",
    "  \\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n",
    "  $\n",
    "  \n",
    "  and substituting $X^T X = I_p$, we get:\n",
    "  $\n",
    "  \\text{Var}(\\hat{\\beta}) = \\sigma^2 I_p\n",
    "  $\n",
    "- This result implies that the variance of each component of $\\hat{\\beta}$ is $\\sigma^2$, and the off-diagonal elements are zero, meaning the components of $\\hat{\\beta}$ are uncorrelated.\n",
    "\n",
    "**Conclusion:**  \n",
    "- In the case of an orthogonal design matrix, the OLS estimator for $\\beta$ simplifies to $\\hat{\\beta} = X^T Y$.\n",
    "- The predicted values $\\hat{Y}$ are exactly equal to the observed values Y, indicating a perfect fit.\n",
    "- The variance of the estimated coefficients $\\hat{\\beta}$ is $\\sigma^2$ for each component, with no correlation between the components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0431e6",
   "metadata": {},
   "source": [
    "### Problem 6: Augmenting Data for Constant Variance in Linear Regression\n",
    "\n",
    "**Objective:**  \n",
    "To explain how to augment the data in a linear regression model when each observation has its own error variance, so that the constant variance assumption of the linear regression model is satisfied.\n",
    "\n",
    "**Given Data:**  \n",
    "- The response vector Y has n observations, and the covariate matrix X is $n \\times p$.\n",
    "- The error terms have a diagonal covariance matrix with diagonal entries $\\sigma_i^2$ for $i = 1, \\ldots, n$. This means each observation $Y_i$ has its own variance $\\sigma_i^2$.\n",
    "\n",
    "**Formula:**  \n",
    "- The linear regression model with heteroscedasticity (non-constant variance) is written as:\n",
    "  $\n",
    "  Y = X\\beta + \\epsilon\n",
    "  $\n",
    "  \n",
    "  where $\\epsilon \\sim N(0, \\Sigma)$ and $\\Sigma$ is a diagonal matrix with entries $\\sigma_1^2, \\sigma_2^2, \\ldots, \\sigma_n^2$.\n",
    "\n",
    "- To achieve homoscedasticity (constant variance), we need to transform the model so that the error terms have equal variance.\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "**Augmenting the Data:**\n",
    "\n",
    "1. **Transform the Model:**\n",
    "   - Define a new vector $Z = D^{-1} Y$ and a new matrix $W = D^{-1} X$, where D is a diagonal matrix with $D_{ii} = \\sigma_i$.\n",
    "   - The transformed model is then:\n",
    "     $\n",
    "     Z = W\\beta + \\epsilon'\n",
    "     $\n",
    "     \n",
    "   where $\\epsilon' = D^{-1} \\epsilon$.\n",
    "\n",
    "2. **Effect on Variance:**\n",
    "   - Since $\\epsilon \\sim N(0, \\Sigma)$, the transformed error term $\\epsilon'$ will have:\n",
    "     $\n",
    "     \\text{Var}(\\epsilon') = D^{-1} \\Sigma D^{-T} = D^{-1} \\Sigma D^{-1} = I_n\n",
    "     $\n",
    "     \n",
    "   - This transformation standardizes the errors so that they now have a constant variance $\\sigma^2$ = 1 (homoscedasticity).\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "- After augmenting the data by dividing each observation by its respective standard deviation $\\sigma_i$, the resulting model has error terms with constant variance.\n",
    "- The transformed response vector $Z = D^{-1} Y$ and design matrix $W = D^{-1} X$ ensure that the assumptions of the linear regression model are now met, specifically the assumption of homoscedasticity.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- By augmenting the data using the transformation $Z = D^{-1} Y$ and $W = D^{-1} X$, where D is a diagonal matrix with the known standard deviations $\\sigma_i$ on the diagonal, we achieve a linear regression model with constant variance.\n",
    "- This approach aligns with techniques discussed in \"A Modern Approach to Regression with R\" and ensures the model adheres to the assumptions necessary for valid inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd661a8",
   "metadata": {},
   "source": [
    "### Problem 7: Regression Diagnostic Plots\n",
    "\n",
    "**Objective:**  \n",
    "To analyze two regression diagnostic plots (residuals vs. fitted values and residual QQ plot) and determine whether they indicate any potential violations of the linear regression model assumptions.\n",
    "\n",
    "**Given Data:**  \n",
    "- Two diagnostic plots are provided: \n",
    "  1. Residuals vs. Fitted Values plot.\n",
    "  2. Residual QQ plot.\n",
    "\n",
    "**Interpretation of Plots:**\n",
    "\n",
    "**1. Residuals vs. Fitted Values Plot:**\n",
    "   - **Purpose:** This plot is used to check for non-linearity, heteroscedasticity (non-constant variance of errors), and the presence of outliers.\n",
    "   - **Interpretation:**\n",
    "     - **Non-linearity:** If the residuals show a systematic pattern (e.g., a curve or trend), it suggests that the linearity assumption may be violated, indicating that a linear model may not be appropriate.\n",
    "     - **Heteroscedasticity:** If the residuals spread out or funnel as the fitted values increase, it suggests that the variance of the errors is not constant (heteroscedasticity), violating one of the key assumptions of linear regression.\n",
    "     - **Outliers:** Large residuals (far from zero) may indicate the presence of outliers, which can disproportionately affect the model fit.\n",
    "\n",
    "**2. Residual QQ Plot:**\n",
    "   - **Purpose:** This plot is used to assess whether the residuals follow a normal distribution, which is an assumption in linear regression for inference purposes.\n",
    "   - **Interpretation:**\n",
    "     - **Normality:** If the residuals follow a straight line on the QQ plot, it suggests that the residuals are normally distributed. Deviations from this line, especially at the tails, suggest non-normality.\n",
    "     - **Heavy Tails or Skewness:** If the residuals deviate significantly from the line, particularly at the ends, it could indicate heavy tails or skewness in the residuals, suggesting a violation of the normality assumption.\n",
    "\n",
    "**Analysis of the Plots:**\n",
    "\n",
    "- **Residuals vs. Fitted Values Plot:**\n",
    "  - If the plot shows no clear pattern and the residuals appear randomly scattered around the horizontal axis, this suggests that the linearity assumption is reasonable, and there is no indication of heteroscedasticity.\n",
    "  - If, however, there is a discernible pattern (such as a curve or fan shape), this suggests potential non-linearity or heteroscedasticity, requiring further investigation or potential model adjustment.\n",
    "\n",
    "- **Residual QQ Plot:**\n",
    "  - If the residuals lie approximately along the 45-degree line, this indicates that the residuals are normally distributed, supporting the validity of the normality assumption.\n",
    "  - Significant deviations from the line, particularly at the ends, would suggest that the residuals are not normally distributed, which could affect the validity of hypothesis tests and confidence intervals.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- **Potential Violations:**\n",
    "  - If the Residuals vs. Fitted Values plot shows a systematic pattern or the spread of residuals increases with fitted values, there may be a violation of the linearity or homoscedasticity assumptions.\n",
    "  - If the Residual QQ plot shows significant deviations from the 45-degree line, particularly at the tails, the normality assumption for the residuals may be violated.\n",
    "\n",
    "- **Actions:**\n",
    "  - If violations are detected, consider transforming the data, adding interaction terms, or using a different model that better captures the relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68072ac",
   "metadata": {},
   "source": [
    "### Problem 8: Analyzing the Scatter Plot and Regression Output\n",
    "\n",
    "**Objective:**  \n",
    "To evaluate the linear regression assumptions using a scatter plot and R regression output, and to interpret specific details provided in the output.\n",
    "\n",
    "**Given Data:**  \n",
    "- A scatter plot of the data and the R regression output are provided.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "**a.) Which of the four linear regression assumptions can be evaluated from the scatter plot? Does this plot suggest those assumptions are violated? Explain.**\n",
    "\n",
    "**Linear Regression Assumptions:**\n",
    "1. **Linearity:** The relationship between the independent and dependent variables should be linear.\n",
    "2. **Independence:** The observations should be independent of each other.\n",
    "3. **Homoscedasticity:** The residuals should have constant variance at every level of the independent variable.\n",
    "4. **Normality:** The residuals should be normally distributed.\n",
    "\n",
    "**Evaluation from the Scatter Plot:**\n",
    "- **Linearity:** The scatter plot allows you to visually assess whether the relationship between the independent variable (x) and the dependent variable (y) is linear. If the points roughly form a straight line, the linearity assumption is likely satisfied.\n",
    "- **Homoscedasticity:** By examining the spread of the points in the scatter plot, you can infer if the variance of the residuals remains constant. If the spread of points increases or decreases as the values of x increase, this suggests heteroscedasticity.\n",
    "- **Outliers:** The scatter plot can also help identify outliers, which may violate the assumptions of linearity and homoscedasticity.\n",
    "\n",
    "**Interpretation:**\n",
    "- If the scatter plot shows a clear linear trend without any funneling or clustering of points, the linearity and homoscedasticity assumptions are likely satisfied.\n",
    "- If there are noticeable deviations from a linear trend or varying spread of points (e.g., increasing spread), these assumptions may be violated.\n",
    "\n",
    "**b.) From the R regression output, how many observations were used to fit the model? Explain how you determined this.**\n",
    "\n",
    "**Answer:**\n",
    "- The number of observations used to fit the model is typically provided in the \"Residuals\" section or in the \"Degrees of Freedom\" section of the R regression output. \n",
    "- Specifically, you can determine the number of observations n by looking at the degrees of freedom (df) associated with the residuals:\n",
    "  $\n",
    "  n = \\text{Residual df} + \\text{Number of estimated parameters}\n",
    "  $\n",
    "  \n",
    "  For example, if the residual degrees of freedom is 98 and two parameters ($\\beta_0$ and $\\beta_1$) are estimated, the total number of observations n would be 98 + 2 = 100.\n",
    "\n",
    "**c.) From the R regression output, is the hypothesis $H_0: \\beta_1$ = 0 significant at the 0.05 level?**\n",
    "\n",
    "**Answer:**\n",
    "- To determine if the hypothesis $H_0: \\beta_1$ = 0 is significant at the 0.05 level, you need to look at the p-value associated with $\\beta_1$ in the R regression output.\n",
    "- If the p-value is less than 0.05, you reject the null hypothesis, indicating that $\\beta_1$ is significantly different from zero.\n",
    "- If the p-value is greater than 0.05, you fail to reject the null hypothesis, meaning there is not enough evidence to say $\\beta_1$ is significantly different from zero.\n",
    "\n",
    "**d.) On the scatter plot, draw the fitted regression line (as best you can, need not be exact). On the line, identify $\\hat{\\beta}_0$ with a $\\Delta$. What are the coordinates of the point?**\n",
    "\n",
    "**Answer:**\n",
    "- The fitted regression line is given by:\n",
    "  $\n",
    "  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "  $\n",
    "  \n",
    "- $\\hat{\\beta}_0$ is the intercept, which is the predicted value of y when x = 0.\n",
    "- The coordinates of the point where the regression line intercepts the y-axis (where x = 0) are (0, $\\hat{\\beta}_0)$.\n",
    "\n",
    "**e.) Write the formula for the confidence interval of $\\beta_1$, plugging in any values that are known from the regression output. You do not need to compute or simplify.**\n",
    "\n",
    "**Formula:**\n",
    "- The confidence interval for $\\beta_1$ is given by:\n",
    "  $\n",
    "  \\text{CI} = \\hat{\\beta}_1 \\pm t_{\\alpha/2, \\nu} \\cdot \\text{SE}(\\hat{\\beta}_1)\n",
    "  $\n",
    "  \n",
    "  where $t_{\\alpha/2, \\nu}$ is the critical value from the t-distribution with $\\nu$ degrees of freedom, and $\\text{SE}(\\hat{\\beta}_1)$ is the standard error of $\\hat{\\beta}_1$.\n",
    "\n",
    "- **Example:** If the output provides $\\hat{\\beta}_1$ = 2.5, $\\text{SE}(\\hat{\\beta}_1)$ = 0.4, and $t_{0.025, 98}$ = 1.984, the confidence interval would be:\n",
    "  $\n",
    "  \\text{CI} = 2.5 \\pm 1.984 \\times 0.4\n",
    "  $\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- **Assumptions:** The scatter plot allows us to assess the linearity and homoscedasticity assumptions, and the R output provides information on the number of observations, significance of the regression coefficient, and confidence intervals.\n",
    "- **Interpretation:** By examining these elements, we can determine whether the regression model meets the necessary assumptions and provides reliable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79974ee4",
   "metadata": {},
   "source": [
    "### Problem 9: Balanced One-Way ANOVA Model\n",
    "\n",
    "**Objective:**  \n",
    "To interpret the value of $\\mu = \\frac{1}{J} \\sum_{j=1}^{J} \\mu_j$ in the context of a balanced one-way ANOVA model and to verify the equality $\\text{SSTotal} = \\text{SSW} + \\text{SSB}$.\n",
    "\n",
    "**Given Data:**  \n",
    "- The balanced one-way ANOVA model is given by:\n",
    "  $\n",
    "  Y_{ij} = \\mu_j + \\epsilon_{ij}\n",
    "  $\n",
    "  \n",
    "  where $\\epsilon_{ij}$ are independent and identically distributed as $N(0, \\sigma^2)$, for $i = 1, \\ldots, I$ and $j = 1, \\ldots, J$.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "**a.) Give a brief interpretation of the value $\\mu = \\frac{1}{J} \\sum_{j=1}^{J} \\mu_j$.**\n",
    "\n",
    "**Answer:**\n",
    "- **Interpretation of $\\mu$:**\n",
    "  - The value $\\mu = \\frac{1}{J} \\sum_{j=1}^{J} \\mu_j$ represents the overall mean of the group means in the balanced one-way ANOVA model.\n",
    "  - It is essentially the grand mean, or the average of the treatment means $\\mu_j$ across all J groups.\n",
    "  - This grand mean $\\mu$ serves as a benchmark for comparing the individual group means $\\mu_j$ to determine if there are significant differences among the groups.\n",
    "\n",
    "**b.) Show that $\\text{SSTotal} = \\text{SSW} + \\text{SSB}$. That is, verify the equality:**\n",
    "\n",
    "$\n",
    "\\sum_{i=1}^{I} \\sum_{j=1}^{J} (Y_{ij} - \\bar{Y}_{\\cdot \\cdot})^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} (Y_{ij} - \\bar{Y}_{\\cdot j})^2 + I \\sum_{j=1}^{J} (\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot})^2\n",
    "$\n",
    "\n",
    "**Formulas:**\n",
    "- **Total Sum of Squares (SSTotal):**\n",
    "  $\n",
    "  \\text{SSTotal} = \\sum_{i=1}^{I} \\sum_{j=1}^{J} (Y_{ij} - \\bar{Y}_{\\cdot \\cdot})^2\n",
    "  $\n",
    "  where $\\bar{Y}_{\\cdot \\cdot} = \\frac{1}{IJ} \\sum_{i=1}^{I} \\sum_{j=1}^{J} Y_{ij}$ is the grand mean of all observations.\n",
    "\n",
    "- **Within-Group Sum of Squares (SSW):**\n",
    "  $\n",
    "  \\text{SSW} = \\sum_{i=1}^{I} \\sum_{j=1}^{J} (Y_{ij} - \\bar{Y}_{\\cdot j})^2\n",
    "  $\n",
    "  where $\\bar{Y}_{\\cdot j} = \\frac{1}{I} \\sum_{i=1}^{I} Y_{ij}$ is the mean of the j-th group.\n",
    "\n",
    "- **Between-Group Sum of Squares (SSB):**\n",
    "  $\n",
    "  \\text{SSB} = I \\sum_{j=1}^{J} (\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot})^2\n",
    "  $\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "1. **Total Sum of Squares Decomposition:**\n",
    "   - The total sum of squares $\\text{SSTotal}$ can be decomposed as follows:\n",
    "     $\n",
    "     \\text{SSTotal} = \\sum_{i=1}^{I} \\sum_{j=1}^{J} (Y_{ij} - \\bar{Y}_{\\cdot \\cdot})^2\n",
    "     $\n",
    "     \n",
    "     which can be expanded as:\n",
    "     $\n",
    "     \\text{SSTotal} = \\sum_{i=1}^{I} \\sum_{j=1}^{J} [(Y_{ij} - \\bar{Y}_{\\cdot j}) + (\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot})]^2\n",
    "     $\n",
    "\n",
    "2. **Expanding the Square:**\n",
    "   - Expanding the square:\n",
    "     $\n",
    "     (Y_{ij} - \\bar{Y}_{\\cdot \\cdot})^2 = (Y_{ij} - \\bar{Y}_{\\cdot j})^2 + 2(Y_{ij} - \\bar{Y}_{\\cdot j})(\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot}) + (\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot})^2\n",
    "     $\n",
    "\n",
    "3. **Summation over All Observations:**\n",
    "   - Sum the first term over all observations:\n",
    "     $\n",
    "     \\sum_{i=1}^{I} \\sum_{j=1}^{J} (Y_{ij} - \\bar{Y}_{\\cdot j})^2 = \\text{SSW}\n",
    "     $\n",
    "     \n",
    "   - The cross-product term, when summed over all observations, is zero:\n",
    "     $\n",
    "     \\sum_{i=1}^{I} \\sum_{j=1}^{J} 2(Y_{ij} - \\bar{Y}_{\\cdot j})(\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot}) = 0\n",
    "     $\n",
    "     \n",
    "     because the within-group deviations $(Y_{ij} - \\bar{Y}_{\\cdot j})$ sum to zero for each group.\n",
    "\n",
    "   - Sum the last term over all observations:\n",
    "     $\n",
    "     \\sum_{i=1}^{I} \\sum_{j=1}^{J} (\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot})^2 = I \\sum_{j=1}^{J} (\\bar{Y}_{\\cdot j} - \\bar{Y}_{\\cdot \\cdot})^2 = \\text{SSB}\n",
    "     $\n",
    "\n",
    "4. **Conclusion:**\n",
    "   - Thus, we have:\n",
    "     $\n",
    "     \\text{SSTotal} = \\text{SSW} + \\text{SSB}\n",
    "     $\n",
    "   - This equality verifies that the total variability in the data (SSTotal) can be partitioned into the variability within groups (SSW) and the variability between groups (SSB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ea921",
   "metadata": {},
   "source": [
    "### Problem 10: Confounding in Multiple Linear Regression\n",
    "\n",
    "**Objective:**  \n",
    "To explain how a confounding relationship can arise in a multiple linear regression setting and why unaccounted confounders are problematic in the analysis of real-world data.\n",
    "\n",
    "**Given Data:**  \n",
    "- The context is a multiple linear regression model where the response variable Y is modeled as a function of several predictor variables.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**1. What is Confounding?**\n",
    "- **Definition:** Confounding occurs when the effect of a predictor variable $X_1$ on the response variable Y is mixed with the effect of another variable $X_2$. This means that $X_2$ influences both $X_1$ and Y, making it difficult to isolate the effect of $X_1$ on Y.\n",
    "- **Example:** Suppose you are studying the effect of exercise ($X_1$) on weight loss (Y), but diet ($X_2$) also influences weight loss and is correlated with exercise habits. If you do not account for diet, the effect of exercise on weight loss might be over- or under-estimated.\n",
    "\n",
    "**2. How Does Confounding Arise in Multiple Linear Regression?**\n",
    "- In a multiple linear regression model, confounding arises when a predictor variable ($X_2$) that affects both the independent variable ($X_1$) and the dependent variable (Y) is omitted from the model. The regression coefficient for $X_1$ will then capture not only the direct effect of $X_1$ on Y but also the effect of $X_2$ on Y that is attributed to $X_1$ because of their correlation.\n",
    "- **Mathematically:** Suppose Y = $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$. If $X_2$ is omitted from the model, the estimated coefficient $\\hat{\\beta}_1$ may be biased because it also reflects the influence of $X_2$ on Y.\n",
    "\n",
    "**3. Why Are Unaccounted Confounders Problematic?**\n",
    "- **Bias in Estimates:** Unaccounted confounders lead to biased estimates of the regression coefficients, meaning that the estimated effects do not reflect the true relationship between the predictors and the response variable.\n",
    "- **Incorrect Inference:** If the confounding variable is not included in the model, the statistical tests and confidence intervals for the regression coefficients may be invalid. This can lead to incorrect conclusions, such as falsely identifying a significant relationship between variables when none exists or missing a true relationship.\n",
    "- **Misleading Predictions:** In predictive modeling, if confounders are not accounted for, the model's predictions may be inaccurate when applied to new data. This is because the model has captured spurious relationships rather than the true underlying effects.\n",
    "  \n",
    "**4. Real-World Example:**\n",
    "- **Healthcare Study:** Imagine a study investigating the relationship between smoking ($X_1$) and lung cancer (Y). Suppose there is a confounder, such as exposure to pollution ($X_2$), which is related to both smoking and lung cancer. If the study does not account for pollution, the effect of smoking on lung cancer may be exaggerated or underestimated.\n",
    "- **Policy Implications:** In policy-making, decisions based on such a flawed analysis might lead to ineffective or harmful interventions because the true risk factors were not properly identified.\n",
    "\n",
    "**Conclusion:**\n",
    "- Confounding is a critical issue in multiple linear regression that can lead to biased estimates, incorrect inference, and misleading predictions if not properly addressed.\n",
    "- To mitigate confounding, it is essential to include all relevant variables in the model and consider methods such as stratification, matching, or using instrumental variables if direct adjustment is not possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
